import streamlit as st
import requests
from bs4 import BeautifulSoup
from docx import Document
from docx.shared import Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.ns import qn
from docx.oxml import OxmlElement
import re
import io
import tempfile
import os

st.set_page_config(page_title="Google Doc Crawler", layout="wide")

st.title("Google Doc to Word Crawler")
st.markdown("Convert 'view only' Google Docs to editable Word documents with formatting (Bold, Highlights, Images, etc.)")

# --- Tutorial & Footer ---

@st.dialog("Tutorial")
def show_tutorial():
    st.markdown("""
    ### User Guide (Tutorial)
    
    **1. Single URL**
    *   Paste the Google Doc link (View only permissions).
    *   Select **Use Custom Filename** if you want to name the file yourself.
    *   Click **Convert & Download**.
    
    **2. Multiple URLs (Batch)**
    *   Paste a list of links (one per line).
    *   The system will download all of them and compress them into a ZIP file.
    
    **3. Google Drive Folder**
    *   Paste the Google Drive folder link (Must be Public).
    *   Click **Scan Folder** to find Google Docs files.
    *   Select **Process these documents** to download.
    """)

if st.button("Tutorial"):
    show_tutorial()

# Helper function to hex to RGB
def hex_to_rgb(hex_color):
    hex_color = hex_color.lstrip('#')
    return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))

# Helper to apply highlight
def add_highlight(run, color_hex):
    # Mapping widely used colors to standard highlighting indices if possible, 
    # but python-docx doesn't support arbitrary RGB highlighting easily.
    # We'll use a standard yellow if it detects yellow, otherwise maybe just ignore or try a best effort.
    # Actually, python-docx supports standard highlighting colors (YELLOW, GREEN, etc).
    # For custom background colors, we need XML manipulation (shading).
    
    tag = run._r
    shd = OxmlElement('w:shd')
    shd.set(qn('w:val'), 'clear')
    shd.set(qn('w:color'), 'auto')
    shd.set(qn('w:fill'), color_hex.replace("#", ""))
    tag.rPr.append(shd)

def parse_style(style_str):
    styles = {}
    if not style_str:
        return styles
    for item in style_str.split(';'):
        if ':' in item:
            key, val = item.split(':', 1)
            styles[key.strip().lower()] = val.strip()
    return styles

def process_node(doc, paragraph, element):
    if element.name == 'span':
        styles = parse_style(element.get('style', ''))
        
        # Check for nested images first
        img = element.find('img')
        if img:
            process_node(doc, paragraph, img)
            return

        text = element.get_text()
        if not text:
            return

        run = paragraph.add_run(text)
        
        # Bold
        if 'font-weight' in styles and ('700' in styles['font-weight'] or 'bold' in styles['font-weight']):
            run.bold = True
        
        # Italic
        if 'font-style' in styles and 'italic' in styles['font-style']:
            run.italic = True
        
        # Underline
        if 'text-decoration' in styles and 'underline' in styles['text-decoration']:
            run.underline = True
            
        # Text Color
        if 'color' in styles:
            color_hex = styles['color']
            if color_hex.startswith('#'):
                try:
                    rgb = hex_to_rgb(color_hex)
                    run.font.color.rgb = RGBColor(rgb[0], rgb[1], rgb[2])
                except:
                    pass

        # Background Color (Highlight)
        if 'background-color' in styles:
            bg_color = styles['background-color']
            if bg_color != 'transparent' and bg_color.startswith('#'):
                add_highlight(run, bg_color)

    elif element.name == 'img':
        src = element.get('src')
        if src:
            try:
                response = requests.get(src)
                if response.status_code == 200:
                    image_stream = io.BytesIO(response.content)
                    # Try to add picture
                    # Adjust width if style has it, otherwise default
                    styles = parse_style(element.get('style', ''))
                    width = None
                    if 'width' in styles:
                        # naive parsing '396.00px'
                        w_str = styles['width'].replace('px', '')
                        try:
                            # 1px ~= 9525 EMUs, or use shared.Pt? 
                            # docx.shared.Inches(1) is 914400
                            # Let's just create simplistic mapping: 1px = 1/96 inch
                            # python-docx uses default size if not specified
                            pass 
                        except:
                            pass
                    
                    paragraph.add_run().add_picture(image_stream, width=None) # Auto size for now
            except Exception as e:
                st.warning(f"Failed to download image: {e}")
    
    elif element.name == None: # NavigableString
        text = str(element)
        if text.strip():
            paragraph.add_run(text)


import zipfile

def extract_title_from_doc(soup):
    # Try to find the title. In mobilebasic, it might be the first bolded text or the title tag.
    # Title tag in html usually: "Doc Title - Google Docs"
    title_tag = soup.find('title')
    if title_tag:
        title = title_tag.get_text().replace(" - Google Docs", "").strip()
        return title
    
    # Fallback to first non-empty paragraph
    first_p = soup.find('p')
    if first_p:
        return first_p.get_text().strip()[:50] # Limit length
    return "untitled_doc"

def crawl_and_get_doc_object(url):
    # Ensure mobilebasic
    if '/edit' in url:
        url = url.split('/edit')[0] + '/mobilebasic'
    elif 'mobilebasic' not in url:
        if '/d/' in url:
             base = url.split('/d/')[1].split('/')[0]
             url = f"https://docs.google.com/document/d/{base}/mobilebasic"
            
    try:
        response = requests.get(url)
        response.raise_for_status()
    except Exception as e:
        # st.error(f"Error fetching URL: {e}")
        return None, None, str(e)

    soup = BeautifulSoup(response.content, 'html.parser')
    title = extract_title_from_doc(soup)
    
    # Create Docx
    doc = Document()
    
    # ... (existing style parsing logic helper functions are global, need to make sure they are accessible or moved)
    # Re-using the logic from previous crawl_and_convert but splitting it to return doc object
    
    elements = soup.body.find_all('p')
    
    # Track if the previous paragraph was empty
    for i, p in enumerate(elements):
        text_content = p.get_text().strip()
        has_img = p.find('img') is not None
        
        if not text_content and not has_img:
            continue

        paragraph = doc.add_paragraph()
        paragraph.paragraph_format.space_after = Pt(0)
        paragraph.paragraph_format.line_spacing = 1.0
        
        p_styles = parse_style(p.get('style', ''))
        if 'text-align' in p_styles:
            align = p_styles['text-align']
            if align == 'center':
                paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            elif align == 'right':
                paragraph.alignment = WD_ALIGN_PARAGRAPH.RIGHT
            elif align == 'justify':
                paragraph.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
        
        for child in p.contents:
            process_node(doc, paragraph, child)
            
    return doc, title, None

# --- Drive Folder Logic ---

def get_drive_folder_id(url):
    # Matches .../folders/FOLDER_ID... or ?id=FOLDER_ID
    # Standard: https://drive.google.com/drive/folders/1Qs0q7Pw-oepNUoz8DaIjhPrXP2gVLAiy
    match = re.search(r'folders/([a-zA-Z0-9_-]+)', url)
    if match:
        return match.group(1)
    match = re.search(r'[?&]id=([a-zA-Z0-9_-]+)', url)
    if match:
        return match.group(1)
    return None

def scan_drive_folder(folder_id):
    # Use the embedded view which is server-side rendered (usually)
    url = f"https://drive.google.com/embeddedfolderview?id={folder_id}"
    try:
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        links = []
        # The embedded view lists items as <a> tags directly or in rows
        for a in soup.find_all('a', href=True):
            href = a['href']
            # We are looking for /document/d/ID or /file/d/ID (if it's a docx viewable)
            # Normal docs: https://docs.google.com/document/d/ID/edit...
            # Drive files: https://drive.google.com/file/d/ID/view...
            
            # Extract ID
            doc_id = None
            if '/document/d/' in href:
                doc_id = href.split('/document/d/')[1].split('/')[0]
                full_link = f"https://docs.google.com/document/d/{doc_id}/edit"
                links.append(full_link)
            elif '/file/d/' in href:
                # Some might be PDFs or images, but user specifically wants docx/docs
                # We can try to crawl them if they are Google Docs wrapped in Drive UI
                # But typically /file/d/ are binary files. 
                # If the user says "From doc gg to docx", they imply Google Docs native format.
                # However, the user also mentioned "MÃ”N Sáº¢N PHá»¤ KHOA YDS 2024.docx" in the drive list.
                # That is a literal .docx file hosted on Drive. 
                # This crawler is designed for NATIVE Google Docs -> HTML -> Docx.
                # It cannot convert a binary .docx hosted on Drive (we would just download it).
                # For now, let's strictly support Google Docs (native).
                # Sidenote: The user said "MÃ”N Sáº¢N PHá»¤ KHOA YDS 2024.docx" was in the list.
                # If it's already a docx, we could just direct download it?
                # For this specific task "Crawler", I will stick to converting Google Docs (the "View Only" native ones).
                pass
                
        return list(set(links)) # Unique
    except Exception as e:
        st.error(f"Error scanning folder: {e}")
        return []

input_mode = st.radio("Input Mode", ["Single URL", "Multiple URLs (Batch)", "Google Drive Folder"])

urls_to_process = []

if input_mode == "Single URL":
    url_input = st.text_input("Google Doc URL", "https://docs.google.com/document/d/1lfQQ4-niDDeErUhWrL-NQ646uirpNBVFNNeBYQ5L35k/edit?tab=t.0")
    if url_input:
        urls_to_process.append(url_input)

elif input_mode == "Multiple URLs (Batch)":
    multi_input = st.text_area("Enter Google Doc URLs (one per line)", height=150)
    if multi_input:
        urls_to_process = [u.strip() for u in multi_input.split('\n') if u.strip()]

elif input_mode == "Google Drive Folder":
    folder_url = st.text_input("Google Drive Folder URL", "https://drive.google.com/drive/folders/1Qs0q7Pw-oepNUoz8DaIjhPrXP2gVLAiy")
    if folder_url and st.button("Scan Folder"):
        folder_id = get_drive_folder_id(folder_url)
        if folder_id:
            with st.spinner(f"Scanning folder ID: {folder_id}..."):
                found_links = scan_drive_folder(folder_id)
                if found_links:
                    st.success(f"Found {len(found_links)} documents!")
                    st.session_state['scanned_links'] = found_links
                else:
                    st.warning("No Google Docs found in this folder (or folder is not public).")
        else:
            st.error("Invalid Folder URL")

    # Use session state to persist scanned links
    if 'scanned_links' in st.session_state:
        st.write("Documents found:")
        st.code('\n'.join(st.session_state['scanned_links']))
        if st.checkbox("Process these documents?", value=True):
            urls_to_process = st.session_state['scanned_links']


use_custom_name = st.checkbox("Use Custom Filename (for Single URL only)")
custom_filename = ""
if use_custom_name and input_mode == "Single URL":
    custom_filename = st.text_input("Output Filename", "converted_doc.docx")

if st.button("Convert & Download"):
    if not urls_to_process:
        st.warning("Please enter at least one URL.")
    else:
        # Placeholder for progress
        progress_text = st.empty()
        
        # Helper to sanitize filename
        def sanitize_filename(name):
            return re.sub(r'[\\/*?:"<>|]', "", name)

        results = []
        
        for i, url in enumerate(urls_to_process):
            progress_text.text(f"Processing ({i+1}/{len(urls_to_process)}): {url}...")
            doc, title, err = crawl_and_get_doc_object(url)
            
            if doc:
                results.append({
                    "doc": doc,
                    "title": title,
                    "original_url": url
                })
            else:
                st.error(f"Failed to process {url}: {err}")

        if results:
            st.success(f"Fully converted {len(results)} documents!")
            
            # If single file
            if len(results) == 1:
                item = results[0]
                bio = io.BytesIO()
                item['doc'].save(bio)
                
                # Fix regex/naming issue
                raw_name = item['title']
                
                if use_custom_name and custom_filename:
                    final_name = custom_filename
                else:
                    final_name = sanitize_filename(raw_name)
                
                # Ensure exactly one extension
                if not final_name.lower().endswith('.docx'):
                    final_name += ".docx"
                
                st.download_button(
                    label=f"Download {final_name}",
                    data=bio.getvalue(),
                    file_name=final_name,
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                )
            
            # If multiple files -> ZIP
            else:
                zip_buffer = io.BytesIO()
                with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zip_file:
                    for item in results:
                        doc_bio = io.BytesIO()
                        item['doc'].save(doc_bio)
                        
                        safe_title = sanitize_filename(item['title'])
                        fname = f"{safe_title}.docx"
                        
                        # Handle duplicate filenames in zip
                        # We can just increment if collision? 
                        # For now, simplistic approach.
                        zip_file.writestr(fname, doc_bio.getvalue())
                
                st.download_button(
                    label="Download All (ZIP)",
                    data=zip_buffer.getvalue(),
                    file_name="converted_docs.zip",
                    mime="application/zip"
                )



st.markdown("<div style='text-align: center; margin-top: 50px;'><a href='https://www.facebook.com/deno.jsr' target='_blank' style='text-decoration: none; color: #888; font-weight: bold;'>Made with ðŸ’– by Tran Cong Toan</a></div>", unsafe_allow_html=True)
